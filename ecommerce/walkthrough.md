# E-commerce Data Pipeline: Phase 1 Walkthrough

This document summarizes the progress made so far in building the real-time e-commerce data lakehouse. We have successfully implemented the **Source**, **Ingestion**, and **Bronze (Raw) Storage** layers.

## üèóÔ∏è Architecture Progress

Current state of the pipeline:
1.  **Source**: Python Event Generator (Active)
2.  **Ingestion**: Kafka Cluster (Active)
3.  **Processing**: Spark Structured Streaming (Active)
4.  **Storage**: MinIO S3-Compatible Storage with Apache Iceberg (Data Landing)

## üõ†Ô∏è Implementation Highlights

### 1. Environment (Docker)
We've spun up a multi-container environment:
- **Kafka & Zookeeper**: For message queuing.
- **Spark (Master/Worker)**: For distributed processing.
- **MinIO**: Our S3-compatible data lake.
- **Airflow & Postgres**: Ready for future orchestration.

### 2. Event Generator
Created a Python script `src/generator/event_generator.py` that simulates user traffic:
- **Schema**: Unique IDs, timestamps, user/session tracking, and multiple event types (`view`, `add_to_cart`, `purchase`).

### 3. Bronze Streaming Job
The script `src/streaming/bronze_streaming.py` performs the primary ingestion:
- Reads live JSON from Kafka.
- Writes to **Apache Iceberg** tables in MinIO.
- Uses **toTable** for schema-enforced landing.

---

## üõë Troubleshooting & Resolutions

During development, we encountered and resolved several technical challenges:

| Problem | Symptom | Resolution |
| :--- | :--- | :--- |
| **Spark Image Crash** | `bitnami/spark:3.4.1 not found` | Bitnami migrated tags. Switched to the rolling `bitnami/spark:3` tag. |
| **Empty Bucket** | `localhost:9000` connection refused in Spark | Spark runs *inside* Docker. Changed endpoints to internal names like `ecommerce-minio:9000`. |
| **Path Missing** | `File not found` inside container | Added volume mapping (`..:/opt/bitnami/spark/app`) to `docker-compose.yml` to share local code. |
| **Version Mismatch** | JAR download failures / Spark errors | Container uses Spark **3.3.0**. Downgraded script packages from `3.5.0` to `3.3.0`. |
| **Silent Failures** | Multiple Spark processes conflicting | Used `docker-compose restart` to clear the environment and ensure a single clean stream. |
| **OffsetOutOfRange** | Streaming job crashing on restart | Kafka retention purged old offsets. Resolved by purging stale checkpoints in MinIO with a custom Spark script. |
| **Missing Gold Data** | CSVs only showing Jan 26 data | Verified Bronze has Feb 3 data. Today's 500+ records were `view_item`/`add_to_cart`. Gold filters for `purchase`. |

## ‚úÖ Proof of Work

### Data Freshness (2026-02-03)
As of the final check:
- **Bronze Records**: 8,970 total (including 500+ from today).
- **Latest Timestamp**: `2026-02-03T18:23:09Z`.
- **Status**: Live ingestion is ACTIVE.

---

## üöÄ Phase 2: Silver & Gold Layers (Completed)

### 4. Silver Layer (Processing)
Implemented `src/batch/silver_batch.py` to:
- Convert timestamps.
- Default null prices.
- Deduplicate events.
- Write to **Iceberg** table `demo.default.silver_events`.

### 5. Gold Layer & Visualization
**Goal**: Aggregate data into business metrics (Hourly Sales, Top Products) and see it in a Dashboard.

1.  **Run Gold Batch Job**:
    ```bash
    docker exec ecommerce-spark-master spark-submit \
      --packages org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.4.2,org.apache.hadoop:hadoop-aws:3.3.4 \
      /opt/bitnami/spark/app/src/batch/gold_batch.py
    ```

2.  **Verify Outputs**:
    Check for the creation of CSV files in the project root:
    - `gold_category_sales.csv`
    - `gold_top_products.csv`

3.  **Check Streamlit Dashboard**:
    - Open [http://localhost:8501](http://localhost:8501)
    - Click **"üèÜ Batch Insights (Gold Layer)"** tab.
    - Charts for Hourly Revenue, Top Products, and Top Users are populated.

### 6. Orchestration (Airflow)
**Goal**: Automate the pipeline.
1.  **Access Airflow**: [http://localhost:8082](http://localhost:8082) (airflow/airflow)
2.  **Trigger DAG**: Unpause and trigger `ecommerce_medallion_pipeline`.
3.  **Monitor**: Watch tasks execute `spark-submit` commands via `DockerOperator`.
Successfully refactored `silver_quality.py` to be compatible with **GX 0.18.3** and added runtime dependency installation (`pip install`) to the DAG.

## ‚è≠Ô∏è Next Steps
- Verify Airflow DAG execution (UI check).
- Implement Data Quality (Great Expectations) blocking logic.
- Add Monitoring (Grafana/Prometheus).

## üèÅ Conclusion
The pipeline is now fully operational, stable, and verified. Fresh data is flowing from the generator through Kafka into the Lakehouse.
